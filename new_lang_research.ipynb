{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dHRQyBuOquy",
        "outputId": "3a63f8bd-b956-4041-ef65-269284a90c9d"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "\n",
        "# \n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOwVisHpNDmF"
      },
      "outputs": [],
      "source": [
        "# pip install cleantext\n",
        "# !pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "lEFLNKMIO9s7",
        "outputId": "1116d5c4-3130-4330-a2ad-459395fa85d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>SearchTerm</th>\n",
              "      <th>emoticonBased</th>\n",
              "      <th>lexiconBased</th>\n",
              "      <th>Annotation 1</th>\n",
              "      <th>Annotation 2</th>\n",
              "      <th>Annotation 3</th>\n",
              "      <th>finalLabel5Classes</th>\n",
              "      <th>finalLabel3Classes</th>\n",
              "      <th>SN(Original Shona Tweet)</th>\n",
              "      <th>ENGoogleTranslate</th>\n",
              "      <th>shona_cleaned</th>\n",
              "      <th>Label5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Education</td>\n",
              "      <td>Vana</td>\n",
              "      <td>POS</td>\n",
              "      <td>NEG</td>\n",
              "      <td>VNEG</td>\n",
              "      <td>VNEG</td>\n",
              "      <td>NEU</td>\n",
              "      <td>VNEG</td>\n",
              "      <td>NEG</td>\n",
              "      <td>@GombaGuru @__vigie ðŸ˜‚ðŸ˜‚ ah mudhara inzwaiwo tsi...</td>\n",
              "      <td>'g oh mammal feel sorry for the kids wod out w...</td>\n",
              "      <td>ðŸ˜‚ðŸ˜‚ ah mudhara inzwaiwo tsitsi vana vatambura k...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Agriculture</td>\n",
              "      <td>kudya</td>\n",
              "      <td>UNK</td>\n",
              "      <td>UNK</td>\n",
              "      <td>NEU</td>\n",
              "      <td>NEU</td>\n",
              "      <td>VPOS</td>\n",
              "      <td>NEU</td>\n",
              "      <td>NEU</td>\n",
              "      <td>@ChinyandeGeorge @ngadziore @nancynjenge @tapc...</td>\n",
              "      <td>'my message is a response to your demand that ...</td>\n",
              "      <td>waigona kuhusunga kana kuti achihuisa pane zva...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sanitation</td>\n",
              "      <td>Vanhu</td>\n",
              "      <td>UNK</td>\n",
              "      <td>POS</td>\n",
              "      <td>NEG</td>\n",
              "      <td>NEU</td>\n",
              "      <td>NEG</td>\n",
              "      <td>NEG</td>\n",
              "      <td>NEG</td>\n",
              "      <td>Munenge muchiseka vanhu vari single imi muchii...</td>\n",
              "      <td>'you are making fun of people who are single y...</td>\n",
              "      <td>munenge muchiseka vanhu vari single imi muchii...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Finance</td>\n",
              "      <td>uyu</td>\n",
              "      <td>UNK</td>\n",
              "      <td>UNK</td>\n",
              "      <td>NEU</td>\n",
              "      <td>NEU</td>\n",
              "      <td>VNEG</td>\n",
              "      <td>NEU</td>\n",
              "      <td>NEU</td>\n",
              "      <td>@Sharonrose918 @habeeb_zw uyu oita sei</td>\n",
              "      <td>'g is how to do'</td>\n",
              "      <td>uyu oita sei</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Home_Affairs</td>\n",
              "      <td>Baba</td>\n",
              "      <td>POS</td>\n",
              "      <td>POS</td>\n",
              "      <td>NEU</td>\n",
              "      <td>VPOS</td>\n",
              "      <td>VPOS</td>\n",
              "      <td>VPOS</td>\n",
              "      <td>POS</td>\n",
              "      <td>@baba_nyenyedzi Am honest and practical questi...</td>\n",
              "      <td>'g am honest and prectical quetical quems perm...</td>\n",
              "      <td>am honest and practical question perm sec . ku...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          topic SearchTerm emoticonBased lexiconBased Annotation 1  \\\n",
              "0     Education       Vana           POS          NEG         VNEG   \n",
              "1   Agriculture      kudya           UNK          UNK          NEU   \n",
              "2   Sanitation       Vanhu           UNK          POS          NEG   \n",
              "3       Finance        uyu           UNK          UNK          NEU   \n",
              "4  Home_Affairs       Baba           POS          POS          NEU   \n",
              "\n",
              "  Annotation 2 Annotation 3 finalLabel5Classes finalLabel3Classes  \\\n",
              "0         VNEG          NEU               VNEG                NEG   \n",
              "1          NEU         VPOS                NEU                NEU   \n",
              "2          NEU          NEG                NEG                NEG   \n",
              "3          NEU         VNEG                NEU                NEU   \n",
              "4         VPOS         VPOS               VPOS                POS   \n",
              "\n",
              "                            SN(Original Shona Tweet)  \\\n",
              "0  @GombaGuru @__vigie ðŸ˜‚ðŸ˜‚ ah mudhara inzwaiwo tsi...   \n",
              "1  @ChinyandeGeorge @ngadziore @nancynjenge @tapc...   \n",
              "2  Munenge muchiseka vanhu vari single imi muchii...   \n",
              "3             @Sharonrose918 @habeeb_zw uyu oita sei   \n",
              "4  @baba_nyenyedzi Am honest and practical questi...   \n",
              "\n",
              "                                   ENGoogleTranslate  \\\n",
              "0  'g oh mammal feel sorry for the kids wod out w...   \n",
              "1  'my message is a response to your demand that ...   \n",
              "2  'you are making fun of people who are single y...   \n",
              "3                                   'g is how to do'   \n",
              "4  'g am honest and prectical quetical quems perm...   \n",
              "\n",
              "                                       shona_cleaned  Label5  \n",
              "0  ðŸ˜‚ðŸ˜‚ ah mudhara inzwaiwo tsitsi vana vatambura k...       4  \n",
              "1  waigona kuhusunga kana kuti achihuisa pane zva...       0  \n",
              "2  munenge muchiseka vanhu vari single imi muchii...       2  \n",
              "3                                       uyu oita sei       0  \n",
              "4  am honest and practical question perm sec . ku...       3  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from cleantext import clean # helps to remove imoji in text\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# \n",
        "def clean( text ):\n",
        "  '''clean tweet texts and remove links, usernamas'''\n",
        "  text = text.lower()\n",
        "  text = ' '.join( text.split() )\n",
        "  text = ' '.join( [ re.sub(\"^@\\w+\", \" \", t) for t in text.split(' ') ] ) # remove usernames\n",
        "  # text = ' '.join( [ re.sub(\"^@\\w+\", \" \", t) for t in text.split(' ') ] ) # remove hashtags\n",
        "  text = ' '.join( [ re.sub(\"^http\\w+\", \" \", t) for t in text.split(' ') ] ) # remove links\n",
        "  # text = clean(text, no_emoji=True)\n",
        "  return ' '.join( text.split() )\n",
        "\n",
        "\n",
        "# make classes\n",
        "def make_label( class_ ):\n",
        "  '''\n",
        "  neu   - 0\n",
        "  pos   - 1\n",
        "  neg   - 2\n",
        "  vpos  - 3\n",
        "  vneg  - 4\n",
        "  '''\n",
        "  class_ = class_.lower()\n",
        "  if class_ == 'vneg': return 4\n",
        "  elif class_ == 'neu': return 0\n",
        "  elif class_ == 'neg': return 2\n",
        "  elif class_ == 'vpos': return 3\n",
        "  elif class_ == 'pos': return 1\n",
        "\n",
        "# feature processing\n",
        "df = pd.read_csv(\"./traindata1.1.csv\",engine=\"python\")\n",
        "df.drop(axis=1, inplace=True, columns=['UserID','Date/Time'] )\n",
        "df.drop_duplicates(inplace=True)\n",
        "# print( df.columns )\n",
        "df['shona_cleaned'] = df['SN(Original Shona Tweet)'].apply( clean ) # clean shona tweets.\n",
        "df['Label5'] = df['finalLabel5Classes'].apply( make_label )\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U2UzN-KGMH1d"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# load and evaluate a saved model\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cu-KMqwHNeCa",
        "outputId": "db40b1b6-35d2-4343-a0aa-2f33f870fd6e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Shona</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9237</th>\n",
              "      <td>pakuzofananidza winky d nezvinhu zvakaita sana...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1762</th>\n",
              "      <td>phone yanga ine mwana ðŸ˜­ðŸ¤£ðŸ¤£</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2865</th>\n",
              "      <td>dembare iri kutamba kwete kana \"ounce\" imwe ch...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5191</th>\n",
              "      <td>manje unozviitwa nani nhai sirivhiya iwe uchit...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4523</th>\n",
              "      <td>wangu vanhu havaite ava tenge tiri kuma terrac...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Shona  Label\n",
              "9237  pakuzofananidza winky d nezvinhu zvakaita sana...      4\n",
              "1762                          phone yanga ine mwana ðŸ˜­ðŸ¤£ðŸ¤£      2\n",
              "2865  dembare iri kutamba kwete kana \"ounce\" imwe ch...      2\n",
              "5191  manje unozviitwa nani nhai sirivhiya iwe uchit...      2\n",
              "4523  wangu vanhu havaite ava tenge tiri kuma terrac...      4"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# preprocessing\n",
        "data = pd.DataFrame()\n",
        "data['Shona'] = df['shona_cleaned']\n",
        "# data['Label'] = LabelEncoder().fit_transform(df['finalLabel5Classes'])\n",
        "data['Label'] = df['Label5']\n",
        "\n",
        "# \n",
        "# X_train, X_test, y_train, y_test = train_test_split(data['Shona'].values, data['Label'].values, test_size=.2, random_state=42 )\n",
        "train, test = train_test_split(data, test_size=.2, random_state=42 )\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHUN__KIS-Ig"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WykjqrPXby5l"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aapoh/codes/ghp/bert-multiclass/envs/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# load bert pre-trained and sequence classifier.\n",
        "# will build model with sequence classifier and tokenizer with bert-tokenizer.\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, InputExample, InputFeatures, TFBertForMultipleChoice\n",
        "num_labels = 5\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTp8bXvGdazv",
        "outputId": "2a186c52-8f1e-4200-ffb9-a246ab884a86"
      },
      "outputs": [],
      "source": [
        "# check summary of bert model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Pp5Zn6c7fEd_",
        "outputId": "c65f02a3-5d2f-4904-a2ee-4bc2194629f1"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ELTVWdUN201Z"
      },
      "outputs": [],
      "source": [
        "# create input sequence.\n",
        "# \n",
        "# InputExample(guid=None, text_a='hello world', text_b=None, label=1)\n",
        "\n",
        "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
        "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                                          text_a = x[DATA_COLUMN], \n",
        "                                                          text_b = None,\n",
        "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
        "  \n",
        "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                                          text_a = x[DATA_COLUMN], \n",
        "                                                          text_b = None,\n",
        "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
        "  return train_InputExamples, validation_InputExamples\n",
        "\n",
        "# train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, 'Shona', 'Label')\n",
        "\n",
        "\n",
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
        "    features = [] # -> will hold InputFeatures to be converted later\n",
        "\n",
        "    for e in examples:\n",
        "        # Documentation is really strong for this method, so please take a look at it\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length, # truncates if len(s) > max_length\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
        "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIwWpI4A51KI",
        "outputId": "e40fcdcb-8991-4f03-c706-81cf13676902"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aapoh/codes/ghp/bert-multiclass/envs/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<RepeatDataset element_spec=({'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run convert examples to dataset function.\n",
        "DATA_COLUMN = 'Shona'\n",
        "LABEL_COLUMN = 'Label'\n",
        "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
        "\n",
        "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
        "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
        "\n",
        "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
        "validation_data = validation_data.batch(32)\n",
        "\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_22BHRhT7fKW",
        "outputId": "0b0ee926-2281-4eaa-f7ca-09fc0a2b4f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-29 13:00:28.105130: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      2/Unknown - 33s 9s/step - loss: 7.1212 - accuracy: 0.1719 "
          ]
        }
      ],
      "source": [
        "# Adam for optimization. categorical crossentropy as the loss function.\n",
        "# sparse categorical accuracy as our accuracy metric.\n",
        "# sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
        "epochs = 2\n",
        "batch = 32\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              \n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
        "              )\n",
        "\n",
        "model.fit(train_data, epochs=epochs, batch_size=batch, validation_data=validation_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "envs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "6b16d1a6559efeda66307266d482d01ef10bc024de7015cc43e9f26f0fe94454"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
